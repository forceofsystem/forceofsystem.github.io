<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer on Force of Life</title>
    <link>https://forceoflife.cn/en/tags/computer/</link>
    <description>Recent content in Computer on Force of Life</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>Zhixiao-Zhang@outlook.com (Patrick)</managingEditor>
    <webMaster>Zhixiao-Zhang@outlook.com (Patrick)</webMaster>
    <lastBuildDate>Sun, 30 Jun 2024 19:06:13 +0800</lastBuildDate><atom:link href="https://forceoflife.cn/en/tags/computer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How CPU accelerates programs?</title>
      <link>https://forceoflife.cn/en/post/jsjzcysjsy/</link>
      <pubDate>Sun, 30 Jun 2024 19:06:13 +0800</pubDate>
      <author>Zhixiao-Zhang@outlook.com (Patrick)</author>
      <guid>https://forceoflife.cn/en/post/jsjzcysjsy/</guid>
      
      <description></description>
      
    </item>
    
    <item>
      <title>Scalar Optimization</title>
      <link>https://forceoflife.cn/en/post/scalar/</link>
      <pubDate>Thu, 14 Mar 2024 02:43:34 +0800</pubDate>
      <author>Zhixiao-Zhang@outlook.com (Patrick)</author>
      <guid>https://forceoflife.cn/en/post/scalar/</guid>
      
      <description>&lt;p&gt;This blog is a note of reading chapter 10 of EAC (Scalar Optimization), which contains not only the content in the book but my understanding. Ultimately, I have finished the reading of the whole book in two months, and will continue to read the dragon book and &lt;em&gt;Advanced Compiler Design and Implementation&lt;/em&gt;. I will also write some notes about those books.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Scalar optimization: opimization of code along a single thread of control.&lt;/p&gt;
&lt;p&gt;Goals of optimizer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rewriting the code into a more efficient or more effective form.&lt;/li&gt;
&lt;li&gt;Producing highly efficient code on a much larger set of inputs.&lt;/li&gt;
&lt;li&gt;Robust&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two machanisms to achieve these goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It eliminates unnescessarys overhead introduced by programming language abstractions, and&lt;/li&gt;
&lt;li&gt;it matches the needs of the resulting program to the available hardware and software resources of the target machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine independent: A transformation that improves code on most target machines is considered machine independent.
Machine dependent: A transformation that relies on knowledge of the target processor is considered machine dependent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Most optimizers are built as a series of passes.&lt;/strong&gt; Each pass takes code in IR form as its input, and produces a rewritten version of the IR code as its output. This structure creates a natural way for the compiler to provide different levels of optimization; each level specifies a set of passes to run.&lt;/p&gt;
&lt;p&gt;We concern, in general, five specific effects of the transformation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eliminate useless and unreachable code&lt;/li&gt;
&lt;li&gt;Move code&lt;/li&gt;
&lt;li&gt;Specialize a computation&lt;/li&gt;
&lt;li&gt;Eliminate a redundant computation&lt;/li&gt;
&lt;li&gt;Enable other transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;death-code-elimination&#34;&gt;Death Code Elimination&lt;/h2&gt;
&lt;p&gt;Useless code (death code): An operationis useless if no operation uses its result, or if all uses of the result are, themselves dead.&lt;/p&gt;
&lt;p&gt;Unreachable code: An operation is unreachable if no valid control-flow path contains the operation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The source of these code: macro expansion or naive translation in the front end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;These code arises in most programs as the direct result of optimization in the compiler.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Branch, or conditionnal branch, is different to jump, or unconditional jump.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;eliminating-useless-code&#34;&gt;Eliminating Useless Code&lt;/h3&gt;
&lt;p&gt;Critical operation: an operation is &lt;em&gt;&lt;strong&gt;critical&lt;/strong&gt;&lt;/em&gt; if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it sets return values for the procedure;&lt;/li&gt;
&lt;li&gt;it’s an input/output statement, or&lt;/li&gt;
&lt;li&gt;it affects the value in a storage location that may be accessible from outside the current procedure (like pointers).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm for eliminating useless code performs two pass over the code, like the mark-sweep gc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first pass:
&lt;ul&gt;
&lt;li&gt;Clear all the mark fields and mark critical operations as “useful”&lt;/li&gt;
&lt;li&gt;Trace the operands of useful operations back to their definitions and marks them as “useful”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;second pass: walk the code and remove any operation not marked.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The algorithm assumes that the code is in SSA form.&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/two-stage.png&#34; alt=&#34;two-stage&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;For control-flow operations, the treatment of the algorithm is more complex than normal operations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Postdominance: In a CFG, &lt;em&gt;j&lt;/em&gt; postdominates &lt;em&gt;i&lt;/em&gt; if and only if every path from i to the exit node passes through j.
Reverse dominance frontier: dominance frontiers computed on the reverse CFG, denoted $RDF(j)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Control-dependence: in a CFG, node j is control-dependent on node i if and only if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There exists a nonnull path from i to j such that j postdominates every node on the path &lt;strong&gt;after i&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;j &lt;strong&gt;doesn’t strictly&lt;/strong&gt; postdominate i.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;In other words, two or more edges leaves block i. One or more edges leads to j and one or more edges do not.
Actually, I think this explanation is easier to understand than the former, but, it’s not a formal definition, hhh.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When &lt;em&gt;Mark&lt;/em&gt; marks an operation in block b as useful, it visits every block in b’s $RDF$ and marks their block-ending branches as useful. (the last part of &lt;em&gt;Mark&lt;/em&gt; routine)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sweep&lt;/em&gt; replaces any unmarked branch with a jump to its first postdominator that contains a marked operation. If the branch is unmarked, then its successors, down to its immediate postdominator, contain no useful operations. (Otherwise, when those operations were marked, the branch would have been marked.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To find the nearest useful postdominator, the algorithm can walk up the postdominator tree until it finds a block that con- tains a useful operation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;eliminating-useless-control-flow&#34;&gt;Eliminating Useless Control Flow&lt;/h3&gt;
&lt;p&gt;If the compiler includes optimizations that can produce useless control flow as a side effect, then it should include a pass that simplifies the cfg by eliminating useless control flow.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;clean&lt;/em&gt; algorithm uses four transformations, which are applyed in the following order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fold a Redundant Branch&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/fold-redundant-branch.png&#34; alt=&#34;fold-redundant&#34; /&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Remove an Empty Block&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/remove-empty-block.png&#34; alt=&#34;remove-empty-block&#34; /&gt;
&lt;/div&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Combine Blocks&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/combine-blocks.png&#34; alt=&#34;combine-blocks&#34; /&gt;
&lt;/div&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Hoist a Branch: If &lt;em&gt;clean&lt;/em&gt; finds a block $B_i$ that ends with a jump to an empty block $B_j$ and $B_j$ ends with a branch. Clean can replace the block-ending jump in $B_i$ with a copy of the branch from $B_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$B_i$ cann’t be empty, and&lt;/li&gt;
&lt;li&gt;$B_i$ cann’t be $B_j$’s sole prodecessor.&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/hoist-branch.png&#34; alt=&#34;hoist-branch&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;A systematic C&lt;em&gt;lean&lt;/em&gt; implementation traverses the graph in postorder, so that $B_i$’s successors are simplified before $B_i$ unless the successor lies along a back edge with respect to the postorder number- ing. This method reduces the number of times that the implementation must move some edges.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/dce.png&#34; alt=&#34;DCE&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;If the CFG contains back edges, then a pass of &lt;em&gt;Clean&lt;/em&gt; may create unprocessed successors along the back edges. For this reason, &lt;em&gt;Clean&lt;/em&gt; repeats the transformation sequence iteratively until the CFG stops changing.&lt;/p&gt;
&lt;p&gt;With the &lt;em&gt;Dead&lt;/em&gt; routine’s help, &lt;em&gt;Clean&lt;/em&gt; can elinimate an empty loop which can’t be done by itself instead of using a specific transformation to handle it.&lt;/p&gt;
&lt;h3 id=&#34;eliminating-unreachable-code&#34;&gt;Eliminating Unreachable Code&lt;/h3&gt;
&lt;p&gt;A block can be unreachable for two distinct reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;there may be no path through the CFG that leads to the block, or&lt;/li&gt;
&lt;li&gt;the paths that reach the block may not be executable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The former case is easy to handle with the same “mark-sweep” algorithm. However, the latter case requires the compiler to reason about the values of expressions that control branches.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the source language allows arithmetic on code pointers or labels (such as C’s pointers), the compiler must preserve all blocks. Otherwise, it can limit the preserved set to blocks whose labels are referenced.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;code-motion&#34;&gt;Code Motion&lt;/h2&gt;
&lt;h3 id=&#34;lazy-code-motion&#34;&gt;Lazy Code Motion&lt;/h3&gt;
&lt;p&gt;LCM performs loop-invariant code motion. It uses data-flow analysis to discover both operations that are candidates for code motion and locations where it can place those operations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The algorithm operates on the IR form of the program and its CFG, rather than on SSA form.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Redundant: an expression $e$ is redundant at p if it has already been evaluated on every path that leads to p.
Partially redundant: an expression $e$ is partially redundant at p if it occurrs on some, but not all, paths that reach p.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LCM combines code motion with elimination of both redundant and partially redundant computations. For the partially redundant expressions, LCM inserts an edge on the path where p doesn’t occur to make the computation fully redundant.&lt;/p&gt;
&lt;h4 id=&#34;code-shape&#34;&gt;&lt;strong&gt;Code Shape&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;LCM moves expression evaluations, not assignments.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Program variables are set by register-to- register copy operations. Variables have lower subscripts than any expression, and that in any operation other than a copy, the defined register’s subscript must be larger than the subscripts of the operation’s arguments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;available-expressions&#34;&gt;&lt;strong&gt;Available Expressions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;LCM needs availability at the end of the block, so it computes $A_{VAIL}O_{UT}$ rather than $A_{VAIL}I_N$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An expression $e$ is available on exit from block $b$ if, along every path from $n_0$ to $b$, $e$ has been evaluated and none of its arguments has been subsequently defined.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LCM computes $A_{VAIL}O_{UT}$ as follows:&lt;/p&gt;
&lt;p&gt;$A_{VAIL}O_{UT}(n_0) = \empty$&lt;/p&gt;
&lt;p&gt;$A_{VAIL}O_{UT}(n) = {\ all\ expressions\ }$, $\forall n ≠ n_0$&lt;/p&gt;
&lt;p&gt;and then iteratively evaluates the following equation until it reaches a fixed point:&lt;/p&gt;
&lt;p&gt;$A_{VAIL}O_{UT}(n) = \mathop{\bigcap}\limits_{m \in preds(n)}(DEE_{XPR}(m) \cup (A_{VAIL}O_{UT}(m) \cap \overline{E_{XPR}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;If an expression $e \in A_{VAIL}O_{UT}(b)$, the compiler could place an evaluation $e$ at the end of block $b$ and obtain the result produced by its most recent evaluation on any control-flow path from $n_0$ to b.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this light, $A_{VAIL}O_{UT}$ sets tell the compiler how far forward in the CFG it can move the evaluation of $e$, ignoring any uses of $e$.&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;anticipable-expressions&#34;&gt;&lt;strong&gt;Anticipable Expressions&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Definition: an expression is anticipable at point $p$ if and only if it is computed on every path that leaves p and produces the same value at each of these compuations.&lt;/p&gt;
&lt;p&gt;The initial condition of the sets is:&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n_f) = \empty$&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n) = {\ all\ expressions\ }$, $\forall n ≠ n_f$&lt;/p&gt;
&lt;p&gt;Next, it solve the follow fixed-point problems:&lt;/p&gt;
&lt;p&gt;$A_{NT}I_{N}(m) = \mathop{\bigcap}\limits_{m \in succ(n)}(UEE_{XPR}(m) \cup (A_{NT}O_{UT}(m) \cap \overline{E_{XPR}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n) = \mathop{\bigcap}\limits_{m \in succ(n)} A_{NT}I_N(m)$, $n ≠ n_f$&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}$ provides information about the safety of hoisting an evaluation to either the start or the end of the current block. If $x \in A_{NT}O_{UT}(b)$, then the compiler can place an evaluation of $x$ at the end of b, with two guarantees:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The evaluation at the end of b will produce the same value as the next evaluation of $x$ along any execution path in the procedure.&lt;/li&gt;
&lt;li&gt;Along any execution path leading out of $b$, the program will evaluate $x$ before redefining any of its arguments.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;earliest-placement&#34;&gt;&lt;strong&gt;Earliest Placement&lt;/strong&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;To simplify the placement equations, LCM assumes that it will place the evaluation on a CFG edge rather than at the start or end of a specific block.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Computing an edge placement lets the compiler defer the decision to place the evaluation &lt;strong&gt;at the end of the edge’s source&lt;/strong&gt;, &lt;strong&gt;at the start of its sink&lt;/strong&gt;, or &lt;strong&gt;in a new block in the middle of the edge&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$E_{ARLIEST}$ set: for a CFG edge &amp;lt;i, j&amp;gt;, an expression $e$ is in $E_{ARLIEST}(i, j)$ if and only if the compiler can legally move $e$ to &amp;lt;i, j&amp;gt;, and cannot move it to any earlier edge in the CFG.&lt;/p&gt;
&lt;p&gt;The $E_{ARLIEST}$ equation encodes this condition as the intersection of three terms:&lt;/p&gt;
&lt;p&gt;$E_{ARLIEST}(i, j) = A_{NT}I_N(j) \cap \overline{A_{VAIL}O_{UT}(i)} \cap (E_{XPR}K_{ILL}(i) \cup \overline{A_{NT}O_{UT}(i)})$&lt;/p&gt;
&lt;p&gt;These terms define an earliest placement for $e$ as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$e \in A_{NT}I_{N}(j)$ means that the compiler can safely move $e$ to the head of $j$.&lt;/li&gt;
&lt;li&gt;$e \notin A_{VAIL}O_{UT}(i)$ shows that no prior computation of $e$ is available on exit from $i$.&lt;/li&gt;
&lt;li&gt;If $e \in E_{XPR}K_{ILL}(i)$, the compiler cann’t move $e$ through block $i$ because of a definition in $i$. If $e \notin A_{NT}O_{UT}(i)$, the compiler cann’t move $e$ into $i$ because $e \notin A_{NT}I_{N}(k)$ for some edge &amp;lt;i, k&amp;gt;. If either is true, then $e$ can move no further than &amp;lt;i, j&amp;gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;later-placement&#34;&gt;Later Placement&lt;/h4&gt;
&lt;p&gt;This problem determines when an earliest placement can be defered to a later point in the CFG while achieving the same effect. It’s formulated as a forward data-flow problem on the CFG with a set $L_{ATER}I_N(n)$ associated with each node and another set $L_{ATER}(i, j)$ associated with each edge &amp;lt;i, j&amp;gt;.&lt;/p&gt;
&lt;p&gt;LCM initializes the $L_{ATER}I_N$ sets as follows:&lt;/p&gt;
&lt;p&gt;$L_{ATER}I_N(n_0) = \empty$&lt;/p&gt;
&lt;p&gt;$L_{ATER}I_N(n) = {\ all\ expressions\ }$, $\forall n ≠ n_0$&lt;/p&gt;
&lt;p&gt;Next, it iteratively computes $L_{ATER}I_N$ and $L_{ATER}$ sets for each block.&lt;/p&gt;
&lt;p&gt;$L_{ATER}I_N(j) = \mathop{\bigcap}\limits_{m \in preds(j)} L_{ATER}(i, j), j \not= n_0$&lt;/p&gt;
&lt;p&gt;$L_{ATER}(i, j) = E_{ARLIEST}(i, j) \cup (L_{ATER}I_N(i) \cap \overline{UEE_{XPR}(i)})$&lt;/p&gt;
&lt;p&gt;An expression $e \in L_{ATER}I_N(k)$ if and only if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;every path that reaches k includes an edge &amp;lt;p, q&amp;gt; such that $e]in E_{ARLIEST}(p, q)$, and&lt;/li&gt;
&lt;li&gt;the path from q to k
&lt;ul&gt;
&lt;li&gt;neither redefines $e’s$ operands&lt;/li&gt;
&lt;li&gt;nor contains an evaluation of $e$ that an earlier placement of $e$ would anticipate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The first condition is that the flow can reach here through the &amp;lt;p, q&amp;gt; where the evaluation can be placed. The second one ensures that the evaluation can produce a correct value and it’s not too late, because if there is an evaluation of $e$ located at the predecessors of $k$, it’s redundant to place another one subsequently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The $E_{ARLIEST}$ term in the equation for $L_{ATER}$ ensures that $L_{ATER}(i,j)$ includes $E_{ARLIEST}(i, j)$. The rest of that equation puts $e$ into $L_{ATER}(i,j)$ if $e$ can be moved forward from $i$ ($e \in L_{ATER}I_N(i)$) and a placement at the entry to i does not anticipate a use in i ($e \notin UEE_{XPR}(i)$).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The former indicates that it’s not too late to put the evaluation into here, the latter implies that the placement is premature because no reference depends on it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Given $L_{ATER}I_N$ and $L_{ATER}$ sets, $e \in L_{ATER}I_N(i)$ implies that the compiler can move the evaluation of $e$ forward through $i$ without losing any benefit; and $e \in L_{ATER}(i, j)$ implies that the compiler can move an evaluation of $e$ in $i$ into $j$.&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;rewriting-the-code&#34;&gt;Rewriting the Code&lt;/h4&gt;
&lt;p&gt;LCM computes two additional sets, $I_{NSERT}$ and $D_{ELETE}$ to drive the rewriting process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The $I_{NSERT}$ set specifies, for each edge, the computations that LCM should insert on that edge. The equation is $I_{NSERT}(i, j) = L_{ATER(i,j)} \cup \overline{L_{ATER}I_N(j)}$.
&lt;ul&gt;
&lt;li&gt;If $i$ has only one successor, LCM can insert the computations at the end of $i$.&lt;/li&gt;
&lt;li&gt;If $j$ has only one predecessor, it can insert the computations at the entry of $j$.&lt;/li&gt;
&lt;li&gt;If neither condition applies, the edge &amp;lt;i, j&amp;gt; is a &lt;strong&gt;critical edge&lt;/strong&gt;, so it should be split by inserting a block in the middle of the edge to evaluate the expressions in $I_{NSERT}(i,j)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The $D_{ELETE}$ set specifies, for a block, which computations LCM should delete from the block. The equation is $D_{ELETE}(i) = UEE_{XPR}(i) \cup \overline{L_{ATER}I_N(i)}$.
&lt;ul&gt;
&lt;li&gt;$D_{ELETE}(n_0)$ is empty, since no block precedes $n_0$.&lt;/li&gt;
&lt;li&gt;If $e \in D_ELETE(i)$, then the first computation of $e$ in $i$ is redundant after all the insertions have been made. Any subsequent evaluation of $e$ in $i$ that has upward-exposed uses—that is, the operands are not defined between the start of $i$ and the evaluation—can also be deleted, the compiler even doesn’t need to rewrite subsequent references.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Recall that LCM doesn’t care about variables, if a register to register copy is unnecessary, subsequent copy coalescing, either in the register allocator or as a standalone pass, should discover that fact and eliminate the copy operation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;code-hoisting&#34;&gt;Code Hoisting&lt;/h3&gt;
&lt;p&gt;Code hoisting provides one direct way of reducing the size of the compiled code. It uses the results of anticipability in a particular simple way.&lt;/p&gt;
&lt;p&gt;If an expression $e \in A_{NT}O_{UT}(b)$, for some block b, that means that $e$ is evaluated along every path that leaves $b$ and evaluating $e$ at the end of $b$ would make the first evaluation along each path redundant. To reduce code size, the compiler can insert an evaluation of $e$ on each path leaving $b$ with a reference to the previously computed value.&lt;/p&gt;
&lt;p&gt;A simple approach has the compiler visit each block $b$ and insert an evaluation of $e$ at the end of $b$, for every expression $e \in A_{NT}O_{UT}(b)$. Subsequent application of LCM or SVN will then remove the newly redundant expressions.&lt;/p&gt;
&lt;h2 id=&#34;specialization&#34;&gt;Specialization&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Major techniques that perform specialization appear in other sections, such as constant propagation, interprocedural constant propagation, operator strength reduction, peephole optimization, and value numbering.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;tail-call-optimization&#34;&gt;Tail-Call Optimization&lt;/h3&gt;
&lt;p&gt;Tail call: the last action that a procedure takes is a call.&lt;/p&gt;
&lt;p&gt;The compiler can specialize tail calls to their contexts in ways that eliminate much of the overhead from the procedure linkage. Supposed that o calls $p$ and $p$ calls $q$, if the call from $p$ to $q$ is a tail call, then no useful computation occurs between the postreturn sequence and the epilogue sequence in $p$. Hence, any code that preserves and restores $p$’s state, beyond what is needed for the return from $p$ to $o$.&lt;/p&gt;
&lt;p&gt;At the call from $p$ to $q$, the minimal precall sequence must evaluate the actual parameters at the call from $p$ to $q$ and adjust the access links or the display if necessary. It need not preserve any caller-saves registers, because they cannot be live. It need not allocate a new AR, because $q$ can use $p$’s AR.&lt;/p&gt;
&lt;p&gt;The tail recursion is a special case of tail call, in which the entire precall sequence devolves to argument evaluation and a branch back to the top of the routine. An eventual return out of the recursion requires &lt;strong&gt;one branch&lt;/strong&gt;, rather than one branch per recursive invocation.&lt;/p&gt;
&lt;h3 id=&#34;leaf-call-optimization&#34;&gt;Leaf-Call Optimization&lt;/h3&gt;
&lt;p&gt;Definition: a procedure that makes no calls.&lt;/p&gt;
&lt;p&gt;During translation of a leaf procedure, the compiler can avoid inserting operations whose sole purpose is to set up for sequent calls such as saving return address from a register into a slot in the AR. In a leaf procedure, the register allocator should try to use caller-saves register before callee-saves registers, which can even keep untouched.&lt;/p&gt;
&lt;p&gt;In addition, the compiler can avoid the runtime overhead of activation-record allocation for leaf procedures.&lt;/p&gt;
&lt;h3 id=&#34;parameter-promotion&#34;&gt;Parameter Promotion&lt;/h3&gt;
&lt;p&gt;Promotion: the compiler proves that an ambiguous value has just one corresponding memory location through detailed analysis of pointer values or array subscript values, or special case analysis. In these cases, it can rewrite the code to move that value into a scalar local variable, where the register allocator can keep it in a register.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To apply this transformation to a procedure p, the optimizer must identify all of the call sites that can invoke p.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;redundancy-elimination&#34;&gt;Redundancy Elimination&lt;/h2&gt;
&lt;p&gt;We’ve learnt three effective techniques for this topic: LVN, SVN, LCM. All three methods differ in the scope of their approach to discovering identical values.&lt;/p&gt;
&lt;h3 id=&#34;value-identity-and-name-identity&#34;&gt;Value Identity and Name Identity&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Value Identity&lt;/strong&gt;: LVM introduced a simple mechanism under SSA form to prove that two expressions had the same value. It assumes that two expressions produce the same value if they have the same operator and their operands have the same value numbers.&lt;/p&gt;
&lt;p&gt;With this rules, LVN can prove that 2 + a has the same value as a + 2 or as 2 + b when a and b have the same value number. However, it can’t prove that a + a and 2 * a has the same value because they have different operators. Similarly, it can’t prove the a + 0 and a has the same value unless we extended it with algebraic identities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Name Identity&lt;/strong&gt;: LCM relies on names to prove that two values have the same number. If LCM sees a + b and a + c, it assumes that they have different values because b and c have different names. We can improve the effectiveness of LCM by encoding value identity into the name space of the code before applying LCM. The distinction between LCM and LVN (or SVN) is clear. Therefore, by encoding value identity into the namespace, the compiler can leverage the strengths of both approaches.&lt;/p&gt;
&lt;h3 id=&#34;dominator-based-value-numbering-dvnt&#34;&gt;Dominator-based Value Numbering (DVNT)&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/DVNT1.png&#34; alt=&#34;DVNT1&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Recall that at the SVN algorithm, we assumes that the block $B_4$ begins with no prior context. However, though $B_4$ can’t rely on values computed in either $B_2$ or $B_3$, it can rely on values computed in $B_0$ and $B_1$, since they occur on every path that reaches $B_4$.&lt;/p&gt;
&lt;p&gt;Fortunately, the SSA name space encodes precisely this distinction. In SSA, a name that is used in some block $B_i$ can only enter $B_i$ in one of two ways. Either the name is defined by a $\phi$-function at the top of $B_i$, or it is defined in some block that dominates $B_i$. Thus, an assignment to x in either $B_2$ or $B_3$ creates a new name for x and forces the insertion of a $\phi$-function for x at the head of $B_4$. Thus, SSA form encodes the presence or absence of an intervening assignment in $B_2$ or $B_3$ directly into the names used in the expression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can use dominance information to locate the most recent predecessor which the algorithm can use.&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/DVNT.png&#34; alt=&#34;DVNT&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;h4 id=&#34;process-the-phi-functions-in-b&#34;&gt;Process the $\phi$-Functions in B&lt;/h4&gt;
&lt;p&gt;If a $\phi$-function $p$ is meaningless, DVNT sets its value number to the value number for one of its arguments and deletes $p$. If $p$ is redundant, DVNT assigns $p$ the same value number as the $\phi$-function that it duplicates, then DVNT deletes $p$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;meaningless: all its arguments have the same value number.
redundant: it produces the same value number as another $\phi$-function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Otherwise, the $\phi$-function computes a new value.&lt;/p&gt;
&lt;h4 id=&#34;process-the-assignments-in-b&#34;&gt;Process the Assignments in B&lt;/h4&gt;
&lt;p&gt;DVNT iterates over the assignments in B and process them in a manner analogous to LVN and SVN. When the algorithm encounters a statement $x ← y\ op\ z$, it can simply replace $y$ with $VN[y]$ because the name in $VN[y]$ holds the same value as y.&lt;/p&gt;
&lt;h4 id=&#34;propagate-information-to-bs-successors&#34;&gt;Propagate Information to B’s Successors&lt;/h4&gt;
&lt;p&gt;Once DVNT has processed all the $\phi$-functions and assignments in $B$, it visits each of B’s CFG successors $s$ and updates $\phi$ function arguments that correspond to values flowing across the edge $(B,s)$. It records the current value number for the argument in the $\phi$-function by overwriting the argument’s SSA name.&lt;/p&gt;
&lt;p&gt;Next, the algorithm recurs on $B$’s children in the dominator tree (follow a preorder walk). Finally, it deallocates the hash table scope that is used for $B$.&lt;/p&gt;
&lt;h2 id=&#34;enabling-other-transformations&#34;&gt;Enabling Other Transformations&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Both loop unrolling and inline substitution obtain most of their benefits by creating context for other optimization.
The tree-height balancing algorithm doesn’t eliminate any operations, but it creates a code shape that can produce better results from instruction scheduling.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;superblock-cloning&#34;&gt;Superblock Cloning&lt;/h3&gt;
&lt;p&gt;In superblock cloning, the optimizer starts with a loop head—the entry to a loop—and clones each path until it reaches a backward branch.&lt;/p&gt;
&lt;p&gt;Superblock cloning can improve the results of optimization in three principal ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It creates longer blocks.&lt;/li&gt;
&lt;li&gt;It eliminates branches.&lt;/li&gt;
&lt;li&gt;It creates points where optimization can occur such as specialization and redundancy elimination.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, cloning has costs, too. It will lead to larger code, which may run more slowly if its size causes additional instruction cache misses.&lt;/p&gt;
&lt;h3 id=&#34;procedure-cloning&#34;&gt;Procedure Cloning&lt;/h3&gt;
&lt;p&gt;The idea is analogous to the block cloning that occurs in superblock cloning. The compiler creates multiple copies of the callee and assigns some of the calls to each instance of the clone&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/original.png&#34; alt=&#34;original&#34; /&gt;
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/after-cloning.png&#34; alt=&#34;after-cloning&#34; /&gt;
&lt;/div&gt;
&lt;h3 id=&#34;loop-unswitching&#34;&gt;Loop Unswitching&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/unswitch.png&#34; alt=&#34;unswitch&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Unswitching is an enabling transformation, it can lead to better scheduling, better register allocation, and fast execution.&lt;/p&gt;
&lt;h3 id=&#34;renaming&#34;&gt;Renaming&lt;/h3&gt;
&lt;p&gt;Renaming is a fertile ground for future work.&lt;/p&gt;
&lt;h2 id=&#34;sparse-conditional-constant-propagation-sccp&#34;&gt;Sparse Conditional Constant Propagation (SCCP)&lt;/h2&gt;
&lt;p&gt;SSCP assigns a lattice value to the operand used by a conditional branch. If the value is neither $\top$ nor $\bot$, then the operand must have a known value and the compiler can rewrite the branch with a jump to one of its two targets, simplifying the CFG. Since this removes an edge from the CFG, it may make the block that was the branch target unreachable. SSCP has no mechanism to take advantage of this knowledge.&lt;/p&gt;
&lt;p&gt;In concept, SCCP operates in a straightforward way. It initializes the data structures. It iterates over two graphs, the CFG and the SSA graph. It propagates reachability information on the CFG and value information on the SSA graph. It halts when the value information reaches a fixed point. Combining these two kinds of information, SCCP can discover both unreachable code and constant values.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To simplify the explanation of SCCP, we assume that each block in the CFG represents just one statement, plus some optional $\phi$-functions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/SCCP1.png&#34; alt=&#34;SCCP1&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;After the initialization phase, the algorithm repeatedly picks an edge from one of the two worklists and process that edge.&lt;/p&gt;
&lt;p&gt;For a CFG edge (m, n), SCCP determines if the edge is marked as executed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If (m, n) is so marked, SCCP take no further action for (m, n).&lt;/li&gt;
&lt;li&gt;If (m, n) is marked as unexecuted, then SCCP marks it as executed and evaluates all of the $\phi$-functions at the start of block $n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, SCCP determines if block $n$ has been previously entered along another edge. If it has not, then SCCP evaluates the assignment or conditional branch in $n$. This processing may add edges to either worklist.&lt;/p&gt;
&lt;p&gt;For an SSA edge, the algorithm first checks if the destination block is reachable. If the block is reachable, SCCP calls one of &lt;em&gt;EvaluatePhi,&lt;/em&gt; EvaluateAssign, or &lt;em&gt;EvaluateConditional&lt;/em&gt;, based on the kind of operation that uses the SSA name.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/SCCP2.png&#34; alt=&#34;SCCP2&#34; style=&#34;width: 49%&#34; /&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/SCCP3.png&#34; alt=&#34;SCCP3&#34; style=&#34;width: 49%&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;After the propagation step, a final pass is required to replace operations that have operands with &lt;em&gt;Value&lt;/em&gt; tags other than $\bot$. The algorithm cannot rewrite the code until the propagation completes.&lt;/p&gt;
&lt;h3 id=&#34;subtleties-in-evaluating-and-rewriting-operations&#34;&gt;Subtleties in Evaluating and Rewriting Operations&lt;/h3&gt;
&lt;p&gt;If the algorithm encounters a multiply operation with operands $\top$ and $\bot$, it might conclude that the operation produces $\bot$. Doing so, however, is premature. Subsequent analysis might lower the $\top$ to the constant 0, so that the multiply produces a value of 0. If SCCP uses the rule $\top \times \bot \rightarrow \bot$, it would increase the running time of SCCP, since the multiply’s value might follow the sequence $\top$, $\bot$, 0. Moreover, it might incorrectly drive other values to $\bot$ and cause SCCP to miss opportunities for improvement.&lt;/p&gt;
&lt;p&gt;To address this, SCCP should use three rules for multiplies that involve $\bot$, as follows: $\top \times \bot \rightarrow \top$, $\alpha \times \bot \rightarrow \bot$ for $\alpha ≠ T$ and $\alpha ≠ 0$, and $0 \times \bot \rightarrow 0$. This same effect occurs for any operation for which the value of one argument can completely determine the result.&lt;/p&gt;
&lt;h3 id=&#34;effectiveness&#34;&gt;Effectiveness&lt;/h3&gt;
&lt;p&gt;sccp can find constants that the sscp algorithm cannot. Similarly, it can discover unreachable code that no combination of the algorithms can discover.&lt;/p&gt;
&lt;h2 id=&#34;strength-reduction&#34;&gt;Strength Reduction&lt;/h2&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Region Constant: a value that doesn’t vary in a loop.
Induction Variable: a value that varies systematically from iteration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Strength reduction looks for contexts in which &lt;strong&gt;an operation&lt;/strong&gt;, such as a multiply, executes inside a loop and its operands are region constant and induction variable. When it finds this situation, it creates a new induction variable that computes the same sequence of values as the original multiplication in a more effcient way.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Candidate Operation: an operation can be reduced with strength reduction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We assume that candidate Operation only has five forms as following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x \leftarrow c \times i$&lt;/li&gt;
&lt;li&gt;$x \leftarrow i \times c$&lt;/li&gt;
&lt;li&gt;$x \leftarrow c + i$&lt;/li&gt;
&lt;li&gt;$x \leftarrow i + c$&lt;/li&gt;
&lt;li&gt;$x \leftarrow i - c$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $c$ is a region constant and $i$ is an induction variable.&lt;/p&gt;
&lt;p&gt;A region constant can either be a literal constant, or a loop-invariant value. With the code in SSA form, we can determine the region constant whether its definition dominate the entry to the loop that defines the induction variable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Performing LCM and constant propagation before strength reduction may expose more region constants.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To make the algorithm effective, we are supposed to give a restricted definition to induction variable: an induction variable is a strongly connected component (SCC) of the SSA graph in which each operation that updates its value is one of&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;an induction variable plus a region constant;&lt;/li&gt;
&lt;li&gt;an induction variable minus a region constant;&lt;/li&gt;
&lt;li&gt;a $\phi$-function, or&lt;/li&gt;
&lt;li&gt;a register-to-register copy from another induction variable.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The SSA graph: In SSA form, each name has a unique definition, so that a name specifies a particular operation in the code that computed its value. We draw SSA graphs with edges that run from a use to its corresponding definition, and the compiler needs to traverse the edges in both directions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consider an operation $o$ of the form $x \leftarrow i \times c$, where $i$ is an induction variable. To test whether $c$ has this property, OSR must relate the SCC for $i$ in the SSA graph back to a loop in the CFG.&lt;/p&gt;
&lt;p&gt;OSR considers the node with the lowest reverse postorder to be the header of the SCC and records that fact in the header field of each node of the SCC.&lt;/p&gt;
&lt;p&gt;In SSA form, the induction variable’s header is the $\phi$-function at the start of the outermost loop in which it varies. In an operation $x \leftarrow i \times c$, where $i$ is an induction variable, $c$ is a region constant if the CFG block that contains its definition dominates the CFG block that contains $i$’s header. To perform this test, the SSA construction must produce a map from each SSA node to the CFG block where it originated.&lt;/p&gt;
&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Tarjan’s Strongly Connected Region Finder:
This is an method to compute the strongly connected component propsed by &lt;a href=&#34;https://en.wikipedia.org/wiki/Robert_Tarjan&#34;&gt;Robert Tarjan&lt;/a&gt;. You can find more information of this &lt;a href=&#34;https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm&#34;&gt;algorithm&lt;/a&gt;. If you feel confused about this method at first like me, you may get further understanding from this video: &lt;a href=&#34;https://www.youtube.com/watch?v=wUgWX0nc4NY&#34;&gt;Tarjan’s Strongly Connected Components&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OSR uses Tarjan’s rtrongly connected region finder to drive the entire process.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/OSR1.png&#34; alt=&#34;OSR1&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;h3 id=&#34;rewriting-the-code-1&#34;&gt;Rewriting the Code&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/scalar-optimization/OSR2.png&#34; alt=&#34;OSR2&#34; style=&#34;zoom: 50%&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Replace&lt;/em&gt; calls &lt;em&gt;Reduce&lt;/em&gt; to rewrite the operation represented by n. Next, it replaces $n$ with a copy operation from the result produced by &lt;em&gt;Replace&lt;/em&gt;. It sets $n$’s header field, and returns.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reduce&lt;/em&gt; and &lt;em&gt;Apply&lt;/em&gt; use a hash table to avoid inserting duplicate operations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduce: first, it calls &lt;em&gt;Clone&lt;/em&gt; to copy the definition for &lt;em&gt;iv&lt;/em&gt;, the induction variable in the operation being reduced. Next, it recurs on the operands of this new operands. An operand defined outside the scc must be either the initial value of &lt;em&gt;iv&lt;/em&gt; or a value by which &lt;em&gt;iv&lt;/em&gt; is incremented. The initial value must be a $\phi$-function argument from outside the SCC. Reduce calls Apply on each such argument.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Apply&lt;/em&gt; takes an opcode and two operands, locates an appropriate point in the code, and inserts that opeartion. It returns the new SSA name for the result of that operation. Normally, &lt;em&gt;Apply&lt;/em&gt; gets a new name, inserts the operation, and returns the result. It locates an appropriate block for the new operation using dominance information. The new operation must go into a block dominated by the blocks that define its operands. If one operand is a constant, &lt;em&gt;Apply&lt;/em&gt; can duplicate the constant in the block that defines the other operand. Otherwise, both operands must have definitions that dominate the header block, and one must dominate the other.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-function-test-replacement-lftr&#34;&gt;Linear-Function Test Replacement (LFTR)&lt;/h3&gt;
&lt;p&gt;Strength reduction often eliminates all uses of an induction variable, except for an end-of-loop test.&lt;/p&gt;
&lt;p&gt;If the compiler can remove this last use, it can eliminate the original induction variable as dead code.&lt;/p&gt;
&lt;p&gt;To perform LFTR, the compiler must&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;locate comparisons that rely on otherwise unneeded induction variables,&lt;/li&gt;
&lt;li&gt;locate an appropriate new induction variable that the comparison could use&lt;/li&gt;
&lt;li&gt;compute the correct region constant for the rewritten test, and&lt;/li&gt;
&lt;li&gt;rewrite tht code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having lftr cooperate with OSR can simplify all of these tasks to produce a fast, effective transformation.&lt;/p&gt;
&lt;p&gt;The operations that lftr targets compare the value of an induction vari- able against a region constant. After OSR finishes its work, lftr should revisit each of these comparisons.&lt;/p&gt;
&lt;p&gt;To facilitate this process, Reduce can record the arithmetic relationship it uses to derive each new induction variable. It can insert a special LFTR edge from each node in the original induction variable to the corresponding node in its reduced counterpart and label it with the operation and region constant of the candidate operation responsible for creating that induction variable.&lt;/p&gt;
&lt;p&gt;When LFTR finds a comparison that should be replaced, it can follow the edges from its induction-variable argument to the final induction variable that resulted from a chain of one or more reductions. The comparison should use this induction variable with an appropriate new region constant.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Data Flow Analysis</title>
      <link>https://forceoflife.cn/en/post/data-flow-analysis/</link>
      <pubDate>Tue, 05 Mar 2024 23:16:20 +0800</pubDate>
      <author>Zhixiao-Zhang@outlook.com (Patrick)</author>
      <guid>https://forceoflife.cn/en/post/data-flow-analysis/</guid>
      
      <description>&lt;p&gt;This is my note for chapter 9 Data Flow Analysis in &lt;em&gt;Engineering A Compiler&lt;/em&gt; (EAC). The parts marked with Confused are the content that I am confused about, including Interprocedural Constant Propagation (Jump Function) and Speeding up the Iterative Dominance Framework. I will write an article to supplement it if I understand it later.&lt;/p&gt;
&lt;h2 id=&#34;introdution&#34;&gt;Introdution&lt;/h2&gt;
&lt;p&gt;Requirements for optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find the points which can be improved&lt;/li&gt;
&lt;li&gt;the transformation is safe&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iterative-data-flow-analysis&#34;&gt;Iterative Data-Flow Analysis&lt;/h2&gt;
&lt;p&gt;For a forward data-flow problem, the algorithm should use an RPO computed on the CFG. By contrast, the algorithm should use an RPO computed on the reverse CFG to solve a backward data-flow problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reverse CFG: the CFG with its edges reversed. Normally, the compiler need to add a unique exit node so that the reverse CFG has a unique entry node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Reverse Postorder (RPO): Label the nodes in the graph with their visitation order in a reverse postorder traversal which visits as many of a node’s predecessors as possible before visiting the node itself. &lt;strong&gt;A node’s RPO number is simple $|N| + 1$ minus its postorder number, where &lt;em&gt;N&lt;/em&gt; is the set of nodes in the graph.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;dominance&#34;&gt;Dominance&lt;/h3&gt;
&lt;p&gt;Definition: node a dominates node b, if and only if a lies on every path from the entry of CFG to b.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The dominance calculation is a forward data-flow problem, and relies only on the structure of the graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$D_{OM} (b_i)$: a set which contains the names of all nodes that dominate $b_i$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using the following equation to compute these sets:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$D_{OM} (n) = {n} \cup \left( \mathop{\bigcap}\limits_{m\in preds(n)} D_{OM} (m) \right)$&lt;/p&gt;
&lt;p&gt;with the initial conditions that $D_{OM} (n_0) = {n_0}$, and $\forall n ≠ n_0$, $D_{OM} (n) = N$, where N is the set of all nodes in the CFG.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Three-step to solve the equation:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;build a CFG&lt;/li&gt;
&lt;li&gt;gather initial information for each block&lt;/li&gt;
&lt;li&gt;solve the equations to produce the $D_{OM}$ sets for each block&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Iterative-Dominance-Framework.png&#34; alt=&#34;Iterative Dominance Framework&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;h3 id=&#34;live-variable-analysis&#34;&gt;Live-Variable Analysis&lt;/h3&gt;
&lt;p&gt;Type: a backward global data-flow problem&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The result of this analysis can be used for register allocation and construction of some variants of SSA form.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Equation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$L_{IVE}O_{UT}(n) = \mathop{\bigcup}\limits_{m \in succ(n)}(UEV_{AR}(m) \cup (L_{IVE}O_{UT}(m) \cap \overline{V_{AR}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;and the initial condition that $L_{IVE}O_{UT}(n) = \emptyset$, $\forall n$.&lt;/p&gt;
&lt;p&gt;Three-step to compute the set:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Perform control-flow analysis to build a CFG.&lt;/li&gt;
&lt;li&gt;Compute the values of the initial sets.&lt;/li&gt;
&lt;li&gt;Apply the iterative algorithm.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Using an RPO computed on the reversed CFG to solve this problem.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The algorithm which computes the sets $L_{IVE}O_{UT}$, it implicitly assumes that execution can reach all of those successors. However, one or more of them may not be reachable.&lt;/li&gt;
&lt;li&gt;Arrays and pointers, which force the analyzer to assume that every variable has been modified. We can solve this problem by guarateening the type safety or analysing the pointer variables.&lt;/li&gt;
&lt;li&gt;Procedure calls. We also have to assume that the callee modifies all the variable which is accessible to both caller and callee and the call-by-reference parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;available-expressions&#34;&gt;Available Expressions&lt;/h3&gt;
&lt;p&gt;Definition: An expression e is available at point p in a procedure if and only if on every path from the procedure’s entry to p, e is evaluated and none of its constituent subexpressions is redefined between that evaluation and p.&lt;/p&gt;
&lt;p&gt;Type: a forward data-flow problem.&lt;/p&gt;
&lt;p&gt;The equation to compute the $A_{VAIL}I_N(n)$:&lt;/p&gt;
&lt;p&gt;$A_{VAIL}I_N(n) = \mathop{\bigcap}\limits_{m \in preds(n)}(DEE_{XPR}(m) \cup (A_{VAIL}I_{N}(m) \cap \overline{E_{XPR}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;, the initial condition that $A_{VAIL}I_N(n_0) = \emptyset$, $A_{VAIL}I_N(n) = {\  all \  expressions \ }, \forall n ≠ n0$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$DEE_{XPR}(n)$: a set of downward exposed expressions in n. An expression $e \in DEE_{XPR}(n)$ if and only if block n evaluates $e$ and none of $e$’s operands is defined between the last evaluation of $e$ in $n$ and the end of $n$.&lt;/li&gt;
&lt;li&gt;$E_{XPR}K_{ILL}(n)$: this set contains all those expressions that are “killed” by  a definition in $n$. An expression is killed if and only if one or more of its operands are redefined in the block.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;An expression $e$ is available on entry to n if and only if it is available on exit from &lt;strong&gt;each&lt;/strong&gt; of $n$’s predecessors in the CFG.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The $A_{VAIL}I_N(n)$ can be used to perform &lt;em&gt;global common subexpression elimination&lt;/em&gt; and lazy code motion.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;reaching-defnitions&#34;&gt;Reaching Defnitions&lt;/h3&gt;
&lt;p&gt;Purpose: to find the set of definitions that reach a block.&lt;/p&gt;
&lt;p&gt;Domain of $R_{EACHES}$ is the set of definitions in procedure.&lt;/p&gt;
&lt;p&gt;A definition d of some variable v reaches operation i if and only if i reads the value of v and there exists a path form d to i that doesn’t define v.&lt;/p&gt;
&lt;p&gt;The compiler annotates each node n in the CFG with a set $R_{EACHES}(n)$, computed as a forward data-flow problem:&lt;/p&gt;
&lt;p&gt;$R_{EACHES}(n) = \emptyset$, $\forall n$&lt;/p&gt;
&lt;p&gt;$R_{EACHES}(n) = \mathop{\bigcup}\limits_{m \in preds(n)} ( DED_{EF}(m) \cup (R_{EACHES}(m) \cap \overline{D_{EF}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;$DED_{EF}(m)$ is the set of downward-exposed definitions in m: those definitions in m for which the defined name is not subsequently redefined in m.&lt;/p&gt;
&lt;p&gt;$D_{EF}K_{ILL}(m)$ contains all the definition points that are obscured by a definition of the same name in m. Thus $\overline{D_{EF}K_{ILL}(m)}$ consists of the definition points that are not obsucred in m.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Computing $DED_{EF}$ and $D_{EF}K_{ILL}$ requires a mapping from names to definiton points. It’s more complex to gather the initial information than it is for live variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;anticipable-expressions&#34;&gt;Anticipable Expressions&lt;/h3&gt;
&lt;p&gt;Definition:&lt;/p&gt;
&lt;p&gt;An expression $e$ is considered &lt;em&gt;anticipable&lt;/em&gt; on exit from block b if and only if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;every path that leaves b evaluates and subsequently use $e$, and&lt;/li&gt;
&lt;li&gt;evaluating $e$ at the end of $b$ would produce the same result as the first evaluation of $e$ along each of those paths. (this property corresponds to the term “anticipable”)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Domainof the problem: the set of expressions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n_f) = \emptyset$&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n) = {\ all \  expressions \ }, \forall n ≠ n_f$&lt;/p&gt;
&lt;p&gt;$A_{NT}O_{UT}(n) = \mathop{\bigcap}\limits_{m \in succ(n)}(UEE_{XPR}(m) \cup (A_{NT}O_{UT}(m) \cap \overline{E_{XPR}K_{ILL}(m)}))$&lt;/p&gt;
&lt;p&gt;$UEE_{XPR}(m)$ is the set of upward-exposed expressions—those used in $m$ before they are killed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The results of anticipability analysis are used in code motion both in lazy code motion and code hoisting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;interprocedural-summary-problems-confused&#34;&gt;Interprocedural Summary Problems (Confused)&lt;/h3&gt;
&lt;p&gt;Problem: In the absence of specific information about the call, the compiler must make worst-case assumptions that account for all the possible actions of the callee, or any procedures that it, in turn, calls.&lt;/p&gt;
&lt;p&gt;To limit such impact, the classic summary problems compute the set of variables that might be modified as a result of the call and that might be used as a result of the call.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;intreprocedural may modify problem&lt;/em&gt; annotates each call site with a set of names that the callee, and procedures it calls, might modify.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The result of this analysis can be used for global constant propagation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Equation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$M_{AY}M_{OD}(p) = L_{OCAL}M_{OD}(p) \cup \left( \mathop{\bigcup}\limits_{e = (p,q)} ubind_e(M_{AY}M_{OD}(q))\right)$&lt;/p&gt;
&lt;p&gt;For a call-graph edge $e = (p,q)$, $unbind_e(x)$ maps each name in x from the name space of q to the name space of p, using the bindings at the specific call site that coreesponds to $e$.&lt;/p&gt;
&lt;p&gt;$L_{OCAL}M_{OD}(p)$ contains all the names modified locally in p that are visible outside p.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It’s computed as the set of names defined in p minus any names that are stricly local to p.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The compiler can also compute information on what variables might be referenced as a result of executing a procedural call, which is called the &lt;em&gt;interprocedural may reference problem&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;static-single-assignment-form&#34;&gt;Static Single-Assignment Form&lt;/h2&gt;
&lt;p&gt;Purpose: to limit the number of analyses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solution: building a variant form that encodes both data flow and control flow directly in the IR.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\phi$-function: this function combines the values from distinct edges. When control enters a block, all the $\phi$-functions in the block execute, &lt;strong&gt;concurrently&lt;/strong&gt;. They evaluate to the argument that corresponds to the edge along which control entered the block.&lt;/p&gt;
&lt;p&gt;Two rules hold after the transformation to SSA:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;each definition in the procedure has a unique name, and&lt;/li&gt;
&lt;li&gt;each use refers to a single definition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps to transform a procedure into SSA form:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insert the appropriate $\phi$-functions for each variable into the code;&lt;/li&gt;
&lt;li&gt;Rename variables with subscripts to make the two rules hold.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Maximal SSA form&lt;/strong&gt;: a naive algorithm to convert IR to SSA, which creates many extraneous $\phi$-funtions.&lt;/p&gt;
&lt;h3 id=&#34;dominator-trees&#34;&gt;Dominator Trees&lt;/h3&gt;
&lt;p&gt;Strictly dominate set: $D_{OM}(n)-n$&lt;/p&gt;
&lt;p&gt;Immediate dominator: the node in strictly dominate set that is closest to n, denoted $ID_{OM}(n)$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The entry node has no immediate dominator.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dominator tree: it contains all node of a flow graph, and there is an edge from m to n if m is $ID_{OM}(n)$.&lt;/p&gt;
&lt;p&gt;The dominator tree contains both the $ID_{OM}$ information and the $D_{OM}$ sets for each node. Each node lies on the path from the root of dominator tree to n, inclusive of both the root and n.&lt;/p&gt;
&lt;h3 id=&#34;dominance-frontiers&#34;&gt;Dominance Frontiers&lt;/h3&gt;
&lt;p&gt;A definition in node n forces a corresponding $\phi$-function at any join point m where&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;n dominates a predecessor of m ($q \in preds(m)$ and $n \in D_{OM}(q)$), and&lt;/li&gt;
&lt;li&gt;n doesn’t strictly dominate m.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dominance frontier: the collection of nodes &lt;em&gt;m&lt;/em&gt; that have above property with respect to n, denoted $DF(n)$.&lt;/p&gt;
&lt;p&gt;The algorithm to compute dominance frontiers is based on three observations as following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Node in a  DF set must be join points in the graph.&lt;/li&gt;
&lt;li&gt;For a join point j, each predecessor k of j must have $j \in DF(k)$.&lt;/li&gt;
&lt;li&gt;If $j \in DF(k)$ for some predecessor k, then j must also be in DF(l) for each $l \in D_{OM}(k)$, unless $l \in D_{OM}(j)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/DF.png&#34; alt=&#34;DF set&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;h3 id=&#34;placing-phi-functions&#34;&gt;Placing $\phi$-Functions&lt;/h3&gt;
&lt;p&gt;To improve the naive algorithm, the basic idea is that a definition of x in block b forces a $\phi$-function at every node in DF(b). But we can continue to improve it, because a variable that is only &lt;strong&gt;live&lt;/strong&gt; within a single block can never have a live $\phi$-function.&lt;/p&gt;
&lt;p&gt;Globals set: a set of names which are live across multiple blocks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In each block, it looks for names with upward-exposed uses — the $UEV_{AR}$ set from the live-variables calculation.&lt;/li&gt;
&lt;li&gt;With the globals set, the compiler can insert $\phi$-functions for those names within it and ignore any name that is not in it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Blocks set: a list of blocks for each name that contain a definition of that name.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These block lists serve as an initial worklist for the $\phi$-insertion algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algorithm to find the sets:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Globals-Blocks.png&#34; alt=&#34;Globals and Blocks&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;and to rewrite the code:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Insertion.png&#34; alt=&#34;Insertion&#34; style=&#34;zoom:75%;&#34; /&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Since all the $\phi$-functions in a block execute concurrently, the order of their insertion is insignificant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the distinction between local names and global names is not sufficient to avoid all dead $\phi$-functions. To further eliminate the extraneous insertions, the compiler can construct $L_{IVE}O_{UT}$ sets and add a test based on liveness to the inner loop of the insertion algorithm. The SSA form generated by that modification is called pruned SSA form.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are two ways to improve the efficiency:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The algorithm should avoid placing any block on the worklist more than once per global name, which can be solved by keeping a checklist of blocks that have already been processed. For implementation, it can use a sparse set to save the space.&lt;/li&gt;
&lt;li&gt;The compiler can maintain a checklist of blocks (a single sparse set, which reinitialized along with WorkList) that already contain $\phi$-functions for a given blocks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;renaming&#34;&gt;Renaming&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Renaming.png&#34; alt=&#34;Renaming&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;This algorithm renames both definitions and uses in a preorder walk over the procedure’s dominator tree. It rewrites the operands with current SSA names, then it creates a new SSA name for the result of the operation. After rewriting the operands and definition, the algorithm rewrites the appropriate $\phi$-function parameters in each CFG successor of the block, using the current SSA names.&lt;/p&gt;
&lt;p&gt;Finally, it recurs on any children of the block in the dominator tree.&lt;/p&gt;
&lt;p&gt;Moreover, the algorithm maintains a stack which holds the subscript of the name’s current SSA name.&lt;/p&gt;
&lt;p&gt;It also maintain a counter to ensure that each definition receives a unique SSA name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The following things makes me confused.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The compiler must assign an ordinal parameter slot in those $\phi$-functions for b. When we draw the SSA form, we always assume a left-to-right order that matches the left-to-right order in which the edges are drawn.
Internally, the compiler can number the edges and parameter slots in any consistent fashion that produces the desired result. This requires cooperation between the code that builds the ssa form and the code that builds the cfg. (For example, if the CFG implementation uses a list of edges leaving each block, the order of that list can determine the mapping.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can use a method to limit the max size of stack to the depth of the dominator tree.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NewName&lt;/em&gt; may overwrite the same stack slot multiple times within a single block. But it requires another mechanism for determining which stacks to pop on exit from a block.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NewName&lt;/em&gt; can thread together the stack entries for a block. Rename can use the thread to pop the appropriate stacks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;translation-out-of-ssa-form&#34;&gt;Translation Out of SSA Form&lt;/h3&gt;
&lt;p&gt;Reason: modern processors do not implement $\phi$-functions. The compiler would produce incorrect code when it simply drops the subscripts, reverts to base names, and deletes the $\phi$-functions if the code has been rearranged.&lt;/p&gt;
&lt;p&gt;The compiler can keep the SSA name space instact and replace each $\phi$-function with a set of copy operations—one along each incoming edge.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Critical Edge: In a CFG, an edge whose source has multiple successors and whose sink has multiple predecessors is called acritical edge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If a block has multiple successors, the compiler cann’t simply insert copies directly into it. To remedy this problem, the compiler can split the edge, insert a new block between the nodes, and place the copies in that new block.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Lost-Copy Problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The lost-copy problem arises from the combination of copy folding and critical edges that cann’t be split.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/out-of-ssa-wrong.png&#34; alt=&#34;Out of SSA Form&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Panel a assigns z the second to last value of i; the code in panel c assigns $z_0$ the last value of i. With the critical edge split, as in panel d, copy insertion produces the correct behavior. However, it adds a jump to every iteration of the loop.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/split-critical.png&#34; alt=&#34;Split Critical Edge&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;The compiler can avoid this problem by checking the liveness of the target name for each copy that it tries to insert during out-of-SSA translation. When it discovers a copy target(in this example is $i_1$) that is live, it must preserve the living value in a tempory name and rewrite subsequent uses to refer to the tempoary name.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This step can be done with an algorithm modelled on the renaming step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/correct-insertion.png&#34; alt=&#34;Correct Insertion&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The swap problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reason: all $\phi$-functions in a block execute concurrently, but the assignments execute serially.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/naive-insertion.png&#34; alt=&#34;Naive Insertion&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;The straightford fix for this problem is to adopt a two-stage copy protocol.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first stage copies each of the $\phi$-function arguments to its own temporary name, simulating the behavior of the original $\phi$-functions.&lt;/li&gt;
&lt;li&gt;The second stage then copies those values to the $\phi$-function targets.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;For the example, it would require four assignments: $s = y_1$, $t = x_1$, $x_1 = s$, $y_1 = t$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, this solution doubles the number of copy operations required to translate out of SSA form.&lt;/p&gt;
&lt;p&gt;In fact, this problem can arise without a cycle of copies. All it takes is a set of $\phi$-functions that have, as inputs, variables defined as outputs of other $\phi$-functions in the same block. Though it can be fixed easily by carefully arranging the order of inserted copies.&lt;/p&gt;
&lt;h3 id=&#34;using-ssa-form-simply-sparse-constant-propagation-algorithm&#34;&gt;Using SSA Form (Simply Sparse Constant Propagation Algorithm)&lt;/h3&gt;
&lt;p&gt;semilattice: A semilattice consists of a set $L$ of values and a meet operator, $\wedge$. The meet operator must be idempotent, commutative and associative; it imposes an order on the elements of $L$ as follows:&lt;/p&gt;
&lt;p&gt;$a ≥ b$ if and only if $a \wedge b = b$ and&lt;/p&gt;
&lt;p&gt;$a &amp;gt; b$ if and only if $a &amp;gt; b$ and $a ≠b$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bottom element: $\bot$, with the properties that $\forall a \in L, a \wedge \bot = \bot, and \ \forall \ a \in L, a ≥ L$&lt;/li&gt;
&lt;li&gt;top element: $\top$, with the properties that $\forall a \in L, a \wedge \top = a, and \ \forall \ a \in L, T ≥ a$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The semilattice for a single SSA name:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Semilattice.png&#34; alt=&#34;Semilattice&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Simply Sparse Constant Propagation Algorithm:&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/SSCP.png&#34; alt=&#34;SSCP Algorithm&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Initialization phase:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If n is defined by a $\phi$-function, SSCP sets &lt;em&gt;Value(n)&lt;/em&gt; to $\top$. (Optimistic)&lt;/li&gt;
&lt;li&gt;If n is a known constant $c_i$, SSCP sets &lt;em&gt;Value(n)&lt;/em&gt; to $c_i$.&lt;/li&gt;
&lt;li&gt;If n’s value cannot be known, SSCP sets &lt;em&gt;Value(n)&lt;/em&gt; to $\bot$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Propagation phase:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For a $\phi$-function, the result is simply the meet of the lattice values of all the $\phi$-function’s arguments.&lt;/li&gt;
&lt;li&gt;For other kinds of operations, the compiler must apply operator-specific knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Because a SSA name can have one of three initial values, so each of them appears on the worklist at most twice.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Optimistic algorithm: algorithm that begin with the value $\top$ rather than $\bot$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In opposition to above, algorithm that begin with the value $\bot$ is ‘pessimistic’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;interprocedural-analysis&#34;&gt;Interprocedural Analysis&lt;/h2&gt;
&lt;p&gt;The inefficiencies introduced by procedure calls appear in two distinct form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;loss of knowledge in single-procedure analysis and optimization that arise from the presence of a call site in the region being anaylzed and transformed, and&lt;/li&gt;
&lt;li&gt;specific overhead introduced to maintain the abstractions inherent in the procedure call.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;call-graph-construction&#34;&gt;Call-Graph Construction&lt;/h3&gt;
&lt;p&gt;Normally, the call-graph construction’s limiting factor is just the cost of scanning procedures to find the call sites. However, some programming language features may make the construction harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;procedure-valued variables: the compiler must analyze the code to estimate the set of potential callees at each call site that invokes a procedure-valued variable. If it just use a simple analog of global constant propagation, it will build extraneous edge. To build the precise call graph, it must track sets of parameters that are passed together, along the same path. The algorithm could then consider each set independently to derive the precise graph.&lt;/li&gt;
&lt;li&gt;contextually-resolved names:
&lt;ul&gt;
&lt;li&gt;inheritance hierarchy in oriented-object;&lt;/li&gt;
&lt;li&gt;a language that allows the program to import either executable code or new class at runtime.
&lt;ul&gt;
&lt;li&gt;To solve this problem, the compiler must construct a conservative call graph that reflects the complete set of potential callees at each call site.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;control-flow graph that has multiple entry and/or exit.
&lt;ul&gt;
&lt;li&gt;$M_{AY}M_{OD}$ analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interprocedural-constant-propagation&#34;&gt;Interprocedural Constant Propagation&lt;/h3&gt;
&lt;p&gt;Goal: Discovering situations where a procedure always receives a known constant value or where a procedure always returns a known constant value.&lt;/p&gt;
&lt;p&gt;Three problems of interprocedural constant propagation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discovering an initial set of constants;
&lt;ul&gt;
&lt;li&gt;simple method: Recognize literal constant values used as parameters.&lt;/li&gt;
&lt;li&gt;more effect and expensive way: Use a full-fledged global constant propation step.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;propagating known constant values around the call graph;
&lt;ul&gt;
&lt;li&gt;This portion of the analysis resembles the iterative data-flow algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;modelling transmission of values through procedures.
&lt;ul&gt;
&lt;li&gt;The compiler builds a small model for each actual parameter, call &lt;em&gt;jump function&lt;/em&gt;. Each jump function, ${J_s}^x$relies on the values of some subset of the formal parameters to the procedure p that contains s, denoted $Support({J_s}^x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algorithm:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/IPO-CP.png&#34; alt=&#34;IPO-CP Algorithm&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Similar to the above propagation algoritm, the worklist should be a sparse set or something like it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Jump Function Implementation (Confused)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The implementation of jump functions is abundant, but all of them must hold following principles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the analyzer determines that parameter x at call site s is a known constant c, then ${J_s}^x = c$, and $Support({J_s}^x) = \emptyset$.&lt;/li&gt;
&lt;li&gt;If $y \in Support({J_s}^x)$ and $Value(y) = \top$, then ${J_s}^x = \top$.&lt;/li&gt;
&lt;li&gt;If the analyzer determines that the value of ${J_s}^x$ cannot be determinded, then ${J_s}^x = \bot$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Two aspects to extend the algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can also construct &lt;em&gt;return jump functions&lt;/em&gt; to model the values returned from callee to caller. The algorithm can treat return jump functions in the same way that it handled ordinary jump functions. And the programmer need to avoid creating cycles of return jump functions that diverge (e.g. for a tail-recursive procedure).&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Return jump functions are particularly important for routines that initialize values such as setting initial values for an object or class in Java.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;To extend the algorithm to cover a larger class of variables, the compiler can simply extend the vector of jump functions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;advanced-topic&#34;&gt;Advanced Topic&lt;/h2&gt;
&lt;h3 id=&#34;speeding-up-the-iterative-domiance-framework-confused&#34;&gt;Speeding up the Iterative Domiance Framework (Confused)&lt;/h3&gt;
&lt;p&gt;We can use the $ID_{OM}$ sets as a proxy for the $D_{OM}$ sets, provided we can provide efficient methods to initialize the sets and to intersect them. We can use a two-pointer algorithm to identify the common suffix.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://forceoflife.cn/2024/data-flow-analysis/images/Advanced-Topic.png&#34; alt=&#34;Advanced Topic&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;This improved algorithm efficient, it halts in two passes on any reducible graph, in more than two passes on any irreducible graph.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
